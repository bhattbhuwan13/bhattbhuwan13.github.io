---
layout: post
title: ResNet - Deep Residual Learning for Image Recognition
---



In a convolutional neural network, convolutional layers learn hiearchical features. The lower layers learn low-level features like edges, corners, etc. and higher layers high level features like mouth, ears, tail,etc. Increasing the network depth or the number of layers should allow the network to learn better features and perform even better but, experimental results show otherwise. <talk about increasing the depth helps network but not all deeper network are easy to optimize>

![errors in deeper network]({{ site.baseurl }}/images/2021-03-10-resnet/error-in-deeper-networks.png)

The graph above compares a 56-layered network with a 26-layered network. The error is higher for deeper network in both cases leading to a lower accuracy.  This drop in accuray is not due to overfitting because the deeper network is performing poorly in both the training and test data. In fact, this poor performance of the deeper network is due to the **degradation problem**. *As the deeper network start converging, accuracy gets saturated and eventually degrades rapidly*. 

This degradation problem indicates that not all networks are easy to optimize. To corroborate this, let us consider two networks:

* a shallower network, and
* its deeper counterpart with few additional layer

If we train the shallow network, copy its weight to the canonical layers of deeper network and the additional layers of the deeper network are just identity mapping, the deeper network should produce no higher training error than the shallower one. But experimental results show that the **deeper network is unable to learn such identity mapping(or unable to do so in feasible time) and get results as good as or better than the shallower network**. 

The easiest way to make your first post is to edit this one. Go into /_posts/ and update the Hello World markdown file. For more instructions head over to the [Jekyll Now repository](https://github.com/barryclark/jekyll-now) on GitHub.

